from selenium.webdriver.common.by import By

class XPathCrawler:
    def __init__(self, driver):
        self.driver = driver

    def crawl(self, param, download_file):
        try:
            element = self.driver.find_element(By.XPATH, param)
            href = element.get_attribute("href")
            if href:
                print(f"[XPath] Found: {href}")
                download_file(href)
            else:
                print("No href found at provided XPath.")
        except Exception as e:
            print(f"[XPath] Error: {e}")

_________________________________________________________________
from selenium.webdriver.common.by import By

class ExtensionCrawler:
    def __init__(self, driver):
        self.driver = driver

    def crawl(self, param, download_file):
        links = self.driver.find_elements(By.TAG_NAME, "a")
        for link in links:
            href = link.get_attribute("href")
            if href and href.lower().endswith(param):
                print(f"[Extension] Found: {href}")
                download_file(href)
                break
________________________________________________________________
from selenium.webdriver.common.by import By

class KeywordCrawler:
    def __init__(self, driver):
        self.driver = driver

    def crawl(self, param, download_file):
        param = param.lower()
        links = self.driver.find_elements(By.TAG_NAME, "a")
        for link in links:
            href = link.get_attribute("href")
            text = link.text.lower()
            if param in text and href:
                print(f"[Keyword] Found: {href}")
                download_file(href)
                break
_________________________________________________________________
import os
import time
import requests
from selenium import webdriver

from xpath import XPathCrawler
from extension import ExtensionCrawler
from keyword import KeywordCrawler


class BaseCrawler:
    def __init__(self, url, download_dir="downloads"):
        self.url = url
        self.download_dir = os.path.abspath(download_dir)
        os.makedirs(self.download_dir, exist_ok=True)

        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        self.driver = webdriver.Chrome(options=options)
        self.driver.get(self.url)
        time.sleep(3)

    def download_file(self, href):
        try:
            filename = os.path.join(self.download_dir, href.split("/")[-1])
            response = requests.get(href)
            with open(filename, "wb") as f:
                f.write(response.content)
            print(f"[Download] Saved to: {filename}")
        except Exception as e:
            print(f"[Download] Failed: {e}")

    def run(self, mode, param):
        try:
            if mode == "xpath":
                crawler = XPathCrawler(self.driver)
            elif mode == "extension":
                crawler = ExtensionCrawler(self.driver)
            elif mode == "keyword":
                crawler = KeywordCrawler(self.driver)
            else:
                print("Invalid mode. Use 'xpath', 'extension', or 'keyword'")
                return

            crawler.crawl(param, self.download_file)
        finally:
            self.driver.quit()


# Example usage
if __name__ == "__main__":
    # Replace these with actual input or use argparse
    mode = "keyword"  # 'xpath' | 'extension' | 'keyword'
    url = "https://example.com"
    param = "2024 annual report"  # or XPath, or ".pdf"

    base = BaseCrawler(url)
    base.run(mode, param)
__________________________________________________________________________

New Base Crawler:
-----------------
import os
import time
import requests
from selenium import webdriver

from xpath import XPathCrawler
from extension import ExtensionCrawler
from keyword import KeywordCrawler

from crawler_request_dto import CrawlRequestDTO


class BaseCrawler:
    def __init__(self, request: CrawlRequestDTO, download_dir="downloads"):
        self.url = str(request.bbase_url)
        self.content_filter = request.content_filter
        self.download_dir = os.path.abspath(download_dir)
        os.makedirs(self.download_dir, exist_ok=True)

        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        self.driver = webdriver.Chrome(options=options)
        self.driver.get(self.url)
        time.sleep(3)

    def download_file(self, href):
        try:
            filename = os.path.join(self.download_dir, href.split("/")[-1])
            response = requests.get(href)
            with open(filename, "wb") as f:
                f.write(response.content)
            print(f"[Download] Saved to: {filename}")
        except Exception as e:
            print(f"[Download] Failed: {e}")

    def run(self):
        try:
            for filter in self.content_filter:
                # Handle XPath
                if filter.xpath:
                    crawler = XPathCrawler(self.driver)
                    for xpath_expr in filter.xpath:
                        crawler.crawl(xpath_expr, self.download_file)

                # Handle file extension
                if filter.file_extension:
                    crawler = ExtensionCrawler(self.driver)
                    for ext in filter.file_extension:
                        crawler.crawl(ext, self.download_file)

                # Handle keyword
                if filter.links_contains_text:
                    crawler = KeywordCrawler(self.driver)
                    for keyword in filter.links_contains_text:
                        crawler.crawl(keyword, self.download_file)
        finally:
            self.driver.quit()
________________________________________________________________________________
main.py
-------
from crawler_request_dto import CrawlRequestDTO, ContentFilter
from base_crawler import BaseCrawler

request = CrawlRequestDTO(
    bbase_url="https://example.com",
    content_filter=[
        ContentFilter(
            file_extension=[".pdf"],
            xpath=["//a[contains(text(),'2024')]"],
            links_contains_text=["2024 annual report"]
        )
    ]
)

crawler = BaseCrawler(request)
crawler.run()
___________________________________________________________________________________


STRATEGY PATTERN:
------------------------------------------------------------------------------------
crawler/
├── base_crawler.py
├── strategy/
│   ├── __init__.py
│   ├── strategy_interface.py      # CrawlStrategy
│   ├── xpath_strategy.py          # XPathStrategy
│   ├── extension_strategy.py      # ExtensionStrategy
│   └── keyword_strategy.py        # KeywordStrategy
├── dto/
│   ├── __init__.py
│   ├── crawler_request_dto.py     # CrawlRequestDTO
│   └── content_filter_dto.py      # ContentFilter


strategy_interface.py
----
from abc import ABC, abstractmethod

class CrawlStrategy(ABC):
    @abstractmethod
    def crawl(self, driver, param, download_callback):
        pass
___________________________________________________________________________________

xpath_strategy.py
----
from selenium.webdriver.common.by import By
from strategy.strategy_interface import CrawlStrategy

class XPathStrategy(CrawlStrategy):
    def crawl(self, driver, param, download_callback):
        for xpath_expr in param:
            try:
                element = driver.find_element(By.XPATH, xpath_expr)
                href = element.get_attribute("href")
                if href:
                    print(f"[XPath] Found: {href}")
                    download_callback(href)
            except Exception as e:
                print(f"[XPath Error] {e}")
_____________________________________________________________________________________
extension_strategy.py
----
from selenium.webdriver.common.by import By
from strategy.strategy_interface import CrawlStrategy

class ExtensionStrategy(CrawlStrategy):
    def crawl(self, driver, param, download_callback):
        links = driver.find_elements(By.TAG_NAME, "a")
        for ext in param:
            for link in links:
                href = link.get_attribute("href")
                if href and href.lower().endswith(ext):
                    print(f"[Extension] Found: {href}")
                    download_callback(href)
                    break
_______________________________________________________________________________________
keyword_strategy.py
----
from selenium.webdriver.common.by import By
from strategy.strategy_interface import CrawlStrategy

class KeywordStrategy(CrawlStrategy):
    def crawl(self, driver, param, download_callback):
        links = driver.find_elements(By.TAG_NAME, "a")
        for keyword in param:
            for link in links:
                text = link.text.lower()
                href = link.get_attribute("href")
                if keyword.lower() in text and href:
                    print(f"[Keyword] Found: {href}")
                    download_callback(href)
                    break
_______________________________________________________________________________________
base_crawler.py
----
import os
import time
import requests
from selenium import webdriver

from strategy.xpath_strategy import XPathStrategy
from strategy.extension_strategy import ExtensionStrategy
from strategy.keyword_strategy import KeywordStrategy

from dto.crawler_request_dto import CrawlRequestDTO


class BaseCrawler:
    def __init__(self, request: CrawlRequestDTO, download_dir="downloads"):
        self.url = str(request.bbase_url)
        self.content_filter = request.content_filter
        self.download_dir = os.path.abspath(download_dir)
        os.makedirs(self.download_dir, exist_ok=True)

        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        self.driver = webdriver.Chrome(options=options)
        self.driver.get(self.url)
        time.sleep(2)

    def download_file(self, href):
        try:
            filename = os.path.join(self.download_dir, href.split("/")[-1].split("?")[0])
            response = requests.get(href)
            with open(filename, "wb") as f:
                f.write(response.content)
            print(f"[Download] Saved: {filename}")
        except Exception as e:
            print(f"[Download Error] {e}")

    def run(self):
        try:
            for filter in self.content_filter:
                if filter.xpath:
                    strategy = XPathStrategy()
                    strategy.crawl(self.driver, filter.xpath, self.download_file)

                if filter.file_extension:
                    strategy = ExtensionStrategy()
                    strategy.crawl(self.driver, filter.file_extension, self.download_file)

                if filter.links_contains_text:
                    strategy = KeywordStrategy()
                    strategy.crawl(self.driver, filter.links_contains_text, self.download_file)
        finally:
            self.driver.quit()

________________________________________________________________________________________

Fat API - main.py
----
from fastapi import FastAPI, HTTPException
from dto.crawler_request_dto import CrawlRequestDTO
from base_crawler import BaseCrawler

app = FastAPI(title="Web Crawler API", version="1.0")


@app.post("/crawl")
async def crawl_site(request: CrawlRequestDTO):
    try:
        crawler = BaseCrawler(request)
        crawler.run()
        return {"message": "Crawling completed successfully."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
_____=____________
import sys
import os
from pathlib import Path
import importlib.util

def load_crawlers():
    crawlers = {}

    if getattr(sys, 'frozen', False):
        base_path = Path(sys._MEIPASS) / "crawlers"
    else:
        base_path = Path(__file__).parent / "crawlers"

    for filename in os.listdir(base_path):
        if filename.startswith("crawler_") and filename.endswith(".py"):
            file_path = base_path / filename
            module_name = filename[:-3]  # e.g. crawler_01_example -> crawler_01_example
            try:
                spec = importlib.util.spec_from_file_location(module_name, file_path)
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                if hasattr(module, "run"):
                    label = module_name.replace("crawler_", "Crawler ").title()
                    crawlers[label] = module.run
            except Exception as e:
                print(f"Failed to import {filename}: {e}")
    return crawlers

# 2:1

import pandas as pd

# Example setup
# df1 = pd.DataFrame({'Entity': ['A', 'B'], 'Name': ['ParentA', 'ParentB'], 'Revenue': [1000, 800]})
# df2 = pd.DataFrame({'Entity': ['C', 'D', 'E'], 'Name': ['ChildC', 'ChildD', 'ChildE'], 'Revenue': [400, 200, 300]})
# mapping_df = pd.DataFrame({'Parent': ['A', 'A'], 'Child': ['C', 'D']})

# Step 1: Merge df2 with mapping to identify parents
df2_merged = df2.merge(mapping_df, left_on='Entity', right_on='Child', how='left')

# Step 2: Replace 'Entity' with parent where applicable
df2_merged['Entity'] = df2_merged['Parent'].fillna(df2_merged['Entity'])

# Step 3: Drop helper columns
df2_merged = df2_merged.drop(columns=['Parent', 'Child'])

# Step 4: Aggregate revenue by new 'Entity'
df2_parent_level = (
    df2_merged.groupby('Entity', as_index=False)
    .agg({'Name': 'first', 'Revenue': 'sum'})
)

# Now df2_parent_level has parent-level revenue
print(df2_parent_level)

# Updated for null entity
import pandas as pd
import numpy as np

# --- Example setup ---
# df1 = pd.DataFrame({'Entity': ['A', 'B'], 'Name': ['ParentA', 'ParentB'], 'Revenue': [1000, 800]})
# df2 = pd.DataFrame({'Entity': ['C', 'D', 'E', '', np.nan],
#                     'Name': ['ChildC', 'ChildD', 'ChildE', 'Unnamed1', 'Unnamed2'],
#                     'Revenue': [400, 200, 300, 150, 250]})
# mapping_df = pd.DataFrame({'Parent': ['A', 'A'], 'Child': ['C', 'D']})

# --- Step 1: Separate rows with and without valid Entity ---
df2_valid = df2[df2['Entity'].notna() & (df2['Entity'].astype(str).str.strip() != '')].copy()
df2_blank = df2[~(df2['Entity'].notna() & (df2['Entity'].astype(str).str.strip() != ''))].copy()

# --- Step 2: Merge valid ones with parent mapping ---
df2_merged = df2_valid.merge(mapping_df, left_on='Entity', right_on='Child', how='left')

# --- Step 3: Replace Entity with parent where applicable ---
df2_merged['Entity'] = df2_merged['Parent'].fillna(df2_merged['Entity'])

# --- Step 4: Drop helper columns ---
df2_merged = df2_merged.drop(columns=['Parent', 'Child'])

# --- Step 5: Aggregate revenue at the new Entity level ---
df2_parent_level = (
    df2_merged.groupby('Entity', as_index=False)
    .agg({'Name': 'first', 'Revenue': 'sum'})
)

# --- Step 6: Add back blank-Entity rows (untouched) ---
df2_final = pd.concat([df2_parent_level, df2_blank], ignore_index=True)

print(df2_final)

