from selenium.webdriver.common.by import By

class XPathCrawler:
    def __init__(self, driver):
        self.driver = driver

    def crawl(self, param, download_file):
        try:
            element = self.driver.find_element(By.XPATH, param)
            href = element.get_attribute("href")
            if href:
                print(f"[XPath] Found: {href}")
                download_file(href)
            else:
                print("No href found at provided XPath.")
        except Exception as e:
            print(f"[XPath] Error: {e}")

_________________________________________________________________
from selenium.webdriver.common.by import By

class ExtensionCrawler:
    def __init__(self, driver):
        self.driver = driver

    def crawl(self, param, download_file):
        links = self.driver.find_elements(By.TAG_NAME, "a")
        for link in links:
            href = link.get_attribute("href")
            if href and href.lower().endswith(param):
                print(f"[Extension] Found: {href}")
                download_file(href)
                break
________________________________________________________________
from selenium.webdriver.common.by import By

class KeywordCrawler:
    def __init__(self, driver):
        self.driver = driver

    def crawl(self, param, download_file):
        param = param.lower()
        links = self.driver.find_elements(By.TAG_NAME, "a")
        for link in links:
            href = link.get_attribute("href")
            text = link.text.lower()
            if param in text and href:
                print(f"[Keyword] Found: {href}")
                download_file(href)
                break
_________________________________________________________________
import os
import time
import requests
from selenium import webdriver

from xpath import XPathCrawler
from extension import ExtensionCrawler
from keyword import KeywordCrawler


class BaseCrawler:
    def __init__(self, url, download_dir="downloads"):
        self.url = url
        self.download_dir = os.path.abspath(download_dir)
        os.makedirs(self.download_dir, exist_ok=True)

        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        self.driver = webdriver.Chrome(options=options)
        self.driver.get(self.url)
        time.sleep(3)

    def download_file(self, href):
        try:
            filename = os.path.join(self.download_dir, href.split("/")[-1])
            response = requests.get(href)
            with open(filename, "wb") as f:
                f.write(response.content)
            print(f"[Download] Saved to: {filename}")
        except Exception as e:
            print(f"[Download] Failed: {e}")

    def run(self, mode, param):
        try:
            if mode == "xpath":
                crawler = XPathCrawler(self.driver)
            elif mode == "extension":
                crawler = ExtensionCrawler(self.driver)
            elif mode == "keyword":
                crawler = KeywordCrawler(self.driver)
            else:
                print("Invalid mode. Use 'xpath', 'extension', or 'keyword'")
                return

            crawler.crawl(param, self.download_file)
        finally:
            self.driver.quit()


# Example usage
if __name__ == "__main__":
    # Replace these with actual input or use argparse
    mode = "keyword"  # 'xpath' | 'extension' | 'keyword'
    url = "https://example.com"
    param = "2024 annual report"  # or XPath, or ".pdf"

    base = BaseCrawler(url)
    base.run(mode, param)
__________________________________________________________________________

New Base Crawler:
-----------------
import os
import time
import requests
from selenium import webdriver

from xpath import XPathCrawler
from extension import ExtensionCrawler
from keyword import KeywordCrawler

from crawler_request_dto import CrawlRequestDTO


class BaseCrawler:
    def __init__(self, request: CrawlRequestDTO, download_dir="downloads"):
        self.url = str(request.bbase_url)
        self.content_filter = request.content_filter
        self.download_dir = os.path.abspath(download_dir)
        os.makedirs(self.download_dir, exist_ok=True)

        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        self.driver = webdriver.Chrome(options=options)
        self.driver.get(self.url)
        time.sleep(3)

    def download_file(self, href):
        try:
            filename = os.path.join(self.download_dir, href.split("/")[-1])
            response = requests.get(href)
            with open(filename, "wb") as f:
                f.write(response.content)
            print(f"[Download] Saved to: {filename}")
        except Exception as e:
            print(f"[Download] Failed: {e}")

    def run(self):
        try:
            for filter in self.content_filter:
                # Handle XPath
                if filter.xpath:
                    crawler = XPathCrawler(self.driver)
                    for xpath_expr in filter.xpath:
                        crawler.crawl(xpath_expr, self.download_file)

                # Handle file extension
                if filter.file_extension:
                    crawler = ExtensionCrawler(self.driver)
                    for ext in filter.file_extension:
                        crawler.crawl(ext, self.download_file)

                # Handle keyword
                if filter.links_contains_text:
                    crawler = KeywordCrawler(self.driver)
                    for keyword in filter.links_contains_text:
                        crawler.crawl(keyword, self.download_file)
        finally:
            self.driver.quit()
________________________________________________________________________________
main.py
-------
from crawler_request_dto import CrawlRequestDTO, ContentFilter
from base_crawler import BaseCrawler

request = CrawlRequestDTO(
    bbase_url="https://example.com",
    content_filter=[
        ContentFilter(
            file_extension=[".pdf"],
            xpath=["//a[contains(text(),'2024')]"],
            links_contains_text=["2024 annual report"]
        )
    ]
)

crawler = BaseCrawler(request)
crawler.run()
___________________________________________________________________________________


STRATEGY PATTERN:
------------------------------------------------------------------------------------
crawler/
├── base_crawler.py
├── strategy/
│   ├── __init__.py
│   ├── strategy_interface.py      # CrawlStrategy
│   ├── xpath_strategy.py          # XPathStrategy
│   ├── extension_strategy.py      # ExtensionStrategy
│   └── keyword_strategy.py        # KeywordStrategy
├── dto/
│   ├── __init__.py
│   ├── crawler_request_dto.py     # CrawlRequestDTO
│   └── content_filter_dto.py      # ContentFilter


strategy_interface.py
----
from abc import ABC, abstractmethod

class CrawlStrategy(ABC):
    @abstractmethod
    def crawl(self, driver, param, download_callback):
        pass
___________________________________________________________________________________

xpath_strategy.py
----
from selenium.webdriver.common.by import By
from strategy.strategy_interface import CrawlStrategy

class XPathStrategy(CrawlStrategy):
    def crawl(self, driver, param, download_callback):
        for xpath_expr in param:
            try:
                element = driver.find_element(By.XPATH, xpath_expr)
                href = element.get_attribute("href")
                if href:
                    print(f"[XPath] Found: {href}")
                    download_callback(href)
            except Exception as e:
                print(f"[XPath Error] {e}")
_____________________________________________________________________________________
extension_strategy.py
----
from selenium.webdriver.common.by import By
from strategy.strategy_interface import CrawlStrategy

class ExtensionStrategy(CrawlStrategy):
    def crawl(self, driver, param, download_callback):
        links = driver.find_elements(By.TAG_NAME, "a")
        for ext in param:
            for link in links:
                href = link.get_attribute("href")
                if href and href.lower().endswith(ext):
                    print(f"[Extension] Found: {href}")
                    download_callback(href)
                    break
_______________________________________________________________________________________
keyword_strategy.py
----
from selenium.webdriver.common.by import By
from strategy.strategy_interface import CrawlStrategy

class KeywordStrategy(CrawlStrategy):
    def crawl(self, driver, param, download_callback):
        links = driver.find_elements(By.TAG_NAME, "a")
        for keyword in param:
            for link in links:
                text = link.text.lower()
                href = link.get_attribute("href")
                if keyword.lower() in text and href:
                    print(f"[Keyword] Found: {href}")
                    download_callback(href)
                    break
_______________________________________________________________________________________
base_crawler.py
----
import os
import time
import requests
from selenium import webdriver

from strategy.xpath_strategy import XPathStrategy
from strategy.extension_strategy import ExtensionStrategy
from strategy.keyword_strategy import KeywordStrategy

from dto.crawler_request_dto import CrawlRequestDTO


class BaseCrawler:
    def __init__(self, request: CrawlRequestDTO, download_dir="downloads"):
        self.url = str(request.bbase_url)
        self.content_filter = request.content_filter
        self.download_dir = os.path.abspath(download_dir)
        os.makedirs(self.download_dir, exist_ok=True)

        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        self.driver = webdriver.Chrome(options=options)
        self.driver.get(self.url)
        time.sleep(2)

    def download_file(self, href):
        try:
            filename = os.path.join(self.download_dir, href.split("/")[-1].split("?")[0])
            response = requests.get(href)
            with open(filename, "wb") as f:
                f.write(response.content)
            print(f"[Download] Saved: {filename}")
        except Exception as e:
            print(f"[Download Error] {e}")

    def run(self):
        try:
            for filter in self.content_filter:
                if filter.xpath:
                    strategy = XPathStrategy()
                    strategy.crawl(self.driver, filter.xpath, self.download_file)

                if filter.file_extension:
                    strategy = ExtensionStrategy()
                    strategy.crawl(self.driver, filter.file_extension, self.download_file)

                if filter.links_contains_text:
                    strategy = KeywordStrategy()
                    strategy.crawl(self.driver, filter.links_contains_text, self.download_file)
        finally:
            self.driver.quit()

________________________________________________________________________________________

Fat API - main.py
----
from fastapi import FastAPI, HTTPException
from dto.crawler_request_dto import CrawlRequestDTO
from base_crawler import BaseCrawler

app = FastAPI(title="Web Crawler API", version="1.0")


@app.post("/crawl")
async def crawl_site(request: CrawlRequestDTO):
    try:
        crawler = BaseCrawler(request)
        crawler.run()
        return {"message": "Crawling completed successfully."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
_____=____________
import sys
import os
from pathlib import Path
import importlib.util

def load_crawlers():
    crawlers = {}

    if getattr(sys, 'frozen', False):
        base_path = Path(sys._MEIPASS) / "crawlers"
    else:
        base_path = Path(__file__).parent / "crawlers"

    for filename in os.listdir(base_path):
        if filename.startswith("crawler_") and filename.endswith(".py"):
            file_path = base_path / filename
            module_name = filename[:-3]  # e.g. crawler_01_example -> crawler_01_example
            try:
                spec = importlib.util.spec_from_file_location(module_name, file_path)
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                if hasattr(module, "run"):
                    label = module_name.replace("crawler_", "Crawler ").title()
                    crawlers[label] = module.run
            except Exception as e:
                print(f"Failed to import {filename}: {e}")
    return crawlers

# 2:1

import pandas as pd

# Example setup
# df1 = pd.DataFrame({'Entity': ['A', 'B'], 'Name': ['ParentA', 'ParentB'], 'Revenue': [1000, 800]})
# df2 = pd.DataFrame({'Entity': ['C', 'D', 'E'], 'Name': ['ChildC', 'ChildD', 'ChildE'], 'Revenue': [400, 200, 300]})
# mapping_df = pd.DataFrame({'Parent': ['A', 'A'], 'Child': ['C', 'D']})

# Step 1: Merge df2 with mapping to identify parents
df2_merged = df2.merge(mapping_df, left_on='Entity', right_on='Child', how='left')

# Step 2: Replace 'Entity' with parent where applicable
df2_merged['Entity'] = df2_merged['Parent'].fillna(df2_merged['Entity'])

# Step 3: Drop helper columns
df2_merged = df2_merged.drop(columns=['Parent', 'Child'])

# Step 4: Aggregate revenue by new 'Entity'
df2_parent_level = (
    df2_merged.groupby('Entity', as_index=False)
    .agg({'Name': 'first', 'Revenue': 'sum'})
)

# Now df2_parent_level has parent-level revenue
print(df2_parent_level)

# Updated for null entity
import pandas as pd
import numpy as np

# --- Example setup ---
# df1 = pd.DataFrame({'Entity': ['A', 'B'], 'Name': ['ParentA', 'ParentB'], 'Revenue': [1000, 800]})
# df2 = pd.DataFrame({'Entity': ['C', 'D', 'E', '', np.nan],
#                     'Name': ['ChildC', 'ChildD', 'ChildE', 'Unnamed1', 'Unnamed2'],
#                     'Revenue': [400, 200, 300, 150, 250]})
# mapping_df = pd.DataFrame({'Parent': ['A', 'A'], 'Child': ['C', 'D']})

# --- Step 1: Separate rows with and without valid Entity ---
df2_valid = df2[df2['Entity'].notna() & (df2['Entity'].astype(str).str.strip() != '')].copy()
df2_blank = df2[~(df2['Entity'].notna() & (df2['Entity'].astype(str).str.strip() != ''))].copy()

# --- Step 2: Merge valid ones with parent mapping ---
df2_merged = df2_valid.merge(mapping_df, left_on='Entity', right_on='Child', how='left')

# --- Step 3: Replace Entity with parent where applicable ---
df2_merged['Entity'] = df2_merged['Parent'].fillna(df2_merged['Entity'])

# --- Step 4: Drop helper columns ---
df2_merged = df2_merged.drop(columns=['Parent', 'Child'])

# --- Step 5: Aggregate revenue at the new Entity level ---
df2_parent_level = (
    df2_merged.groupby('Entity', as_index=False)
    .agg({'Name': 'first', 'Revenue': 'sum'})
)

# --- Step 6: Add back blank-Entity rows (untouched) ---
df2_final = pd.concat([df2_parent_level, df2_blank], ignore_index=True)

print(df2_final)

-------------------/--------------------
import pandas as pd

# -----------------------------
# 1. Merge all three (respect df1)
# -----------------------------
merged = (
    df1
    .merge(df2, on="Name", how="left", suffixes=("", "_df2"))
    .merge(df3, on="Name", how="left", suffixes=("", "_df3"))
)

# ------------------------------------------------------
# 2. Rows in df1 NOT present in df2   (left anti-join)
# ------------------------------------------------------
df1_not_in_df2 = df1.merge(df2[['Name']], on='Name', how='left', indicator=True)
df1_not_in_df2 = df1_not_in_df2[df1_not_in_df2['_merge'] == 'left_only'].drop(columns=['_merge'])

# ------------------------------------------------------
# 3. Rows in df1 NOT present in df3   (left anti-join)
# ------------------------------------------------------
df1_not_in_df3 = df1.merge(df3[['Name']], on='Name', how='left', indicator=True)
df1_not_in_df3 = df1_not_in_df3[df1_not_in_df3['_merge'] == 'left_only'].drop(columns=['_merge'])

# ------------------------------------------------------
# 4. Rows in df2 NOT present in df1   (new rows in df2)
# ------------------------------------------------------
df2_not_in_df1 = df2.merge(df1[['Name']], on='Name', how='left', indicator=True)
df2_not_in_df1 = df2_not_in_df1[df2_not_in_df1['_merge'] == 'left_only'].drop(columns=['_merge'])

# ------------------------------------------------------
# 5. Rows in df3 NOT present in df1   (new rows in df3)
# ------------------------------------------------------
df3_not_in_df1 = df3.merge(df1[['Name']], on='Name', how='left', indicator=True)
df3_not_in_df1 = df3_not_in_df1[df3_not_in_df1['_merge'] == 'left_only'].drop(columns=['_merge'])

# ------------------------------------------------------
# 6. Summary sheet
# ------------------------------------------------------
summary = pd.DataFrame({
    "Description": [
        "Rows in df1 not in df2",
        "Rows in df1 not in df3",
        "Rows in df2 not in df1",
        "Rows in df3 not in df1"
    ],
    "Count": [
        len(df1_not_in_df2),
        len(df1_not_in_df3),
        len(df2_not_in_df1),
        len(df3_not_in_df1)
    ]
})

health check -----------------

"""
website_health_check.py
-----------------------
Fast async URL health-check system.
Use this before running your web crawlers.

Features:
- async parallel requests
- timeout handling
- SSL verification toggles
- automatic redirect detection
- structured output for crawler pre-check
- pydantic-based configuration
"""

import asyncio
import httpx
from typing import List, Optional
from pydantic import BaseModel, HttpUrl, ValidationError
from datetime import datetime


# -----------------------------------------
# CONFIG MODEL
# -----------------------------------------

class URLItem(BaseModel):
    url: HttpUrl
    allow_redirects: bool = True
    timeout: float = 10.0
    verify_ssl: bool = True


class HealthCheckConfig(BaseModel):
    urls: List[URLItem]
    concurrency_limit: int = 10


# -----------------------------------------
# HEALTH CHECK LOGIC
# -----------------------------------------

async def check_url(client: httpx.AsyncClient, item: URLItem) -> dict:
    """Check a single URL and return a structured result."""
    start_time = datetime.utcnow()

    try:
        response = await client.get(
            item.url,
            timeout=item.timeout,
            follow_redirects=item.allow_redirects
        )

        return {
            "url": item.url,
            "status": response.status_code,
            "ok": response.is_success,
            "reason": response.reason_phrase,
            "redirected": response.history != [],
            "final_url": str(response.url),
            "elapsed_ms": response.elapsed.total_seconds() * 1000,
            "error": None,
            "checked_at": start_time.isoformat()
        }

    except httpx.ConnectTimeout:
        return {
            "url": item.url,
            "status": None,
            "ok": False,
            "reason": "timeout",
            "redirected": False,
            "final_url": None,
            "elapsed_ms": None,
            "error": "Connection timeout",
            "checked_at": start_time.isoformat()
        }

    except httpx.ReadTimeout:
        return {
            "url": item.url,
            "status": None,
            "ok": False,
            "reason": "timeout",
            "redirected": False,
            "final_url": None,
            "elapsed_ms": None,
            "error": "Read timeout",
            "checked_at": start_time.isoformat()
        }

    except httpx.ConnectError as e:
        return {
            "url": item.url,
            "status": None,
            "ok": False,
            "reason": "connection_error",
            "redirected": False,
            "final_url": None,
            "elapsed_ms": None,
            "error": str(e),
            "checked_at": start_time.isoformat()
        }

    except httpx.HTTPError as e:
        return {
            "url": item.url,
            "status": None,
            "ok": False,
            "reason": "http_error",
            "redirected": False,
            "final_url": None,
            "elapsed_ms": None,
            "error": str(e),
            "checked_at": start_time.isoformat()
        }

    except Exception as e:
        return {
            "url": item.url,
            "status": None,
            "ok": False,
            "reason": "unknown_error",
            "redirected": False,
            "final_url": None,
            "elapsed_ms": None,
            "error": str(e),
            "checked_at": start_time.isoformat()
        }


# -----------------------------------------
# RUNNER
# -----------------------------------------

async def run_health_check(config: HealthCheckConfig) -> List[dict]:
    """Runs checks concurrently and returns results."""
    semaphore = asyncio.Semaphore(config.concurrency_limit)

    async with httpx.AsyncClient(verify=True) as client:

        async def worker(item: URLItem):
            async with semaphore:
                # Override SSL verification per URL
                client._verify = item.verify_ssl
                return await check_url(client, item)

        tasks = [worker(url) for url in config.urls]
        return await asyncio.gather(*tasks)


# -----------------------------------------
# SAMPLE USAGE
# -----------------------------------------

if __name__ == "__main__":

    # EXAMPLE URLs (use your actual list)
    test_urls = [
        # 200
        {"url": "https://httpstat.us/200"},

        # Redirect
        {"url": "https://httpstat.us/301"},

        # Client errors
        {"url": "https://httpstat.us/404"},
        {"url": "https://httpstat.us/400"},

        # Server errors
        {"url": "https://httpstat.us/500"},
        {"url": "https://httpstat.us/503"},
        {"url": "https://httpstat.us/504?sleep=3000", "timeout": 2},

        # SSL failure examples
        {"url": "https://expired.badssl.com", "verify_ssl": False},
        {"url": "https://self-signed.badssl.com", "verify_ssl": False},
    ]

    try:
        config = HealthCheckConfig(urls=[URLItem(**u) for u in test_urls])
    except ValidationError as ve:
        print("Config validation error:", ve)
        exit(1)

    results = asyncio.run(run_health_check(config))

    print("\n=== Website Health Check Report ===\n")
    for r in results:
        print(r)
