# Below is a Python script that handles multiple websites using Selenium and Python. Each website's scraping logic is stored in a function.
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
import time

# List of websites and their configurations
websites = [
    {
        "name": "Site A",
        "url": "https://www.example-a.com/financial-reports",
        "report_keyword": "Quarterly Report",  # Keyword to search for
    },
    {
        "name": "Site B",
        "url": "https://www.example-b.com/reports",
        "report_keyword": "Financial Statement",
    },
    # Add more sites here
]

# Path to ChromeDriver
driver_path = "path_to_chromedriver"  # Replace with the path to your ChromeDriver
service = Service(driver_path)
driver = webdriver.Chrome(service=service)

def scrape_site(site_config):
    """Scrape a specific site for reports."""
    print(f"Scraping {site_config['name']}...")
    driver.get(site_config["url"])
    time.sleep(5)  # Adjust based on page load time

    try:
        # Search for the keyword in links
        elements = driver.find_elements(By.PARTIAL_LINK_TEXT, site_config["report_keyword"])
        if elements:
            print(f"Reports found on {site_config['name']}:")
            for element in elements:
                report_title = element.text
                report_link = element.get_attribute("href")
                print(f"- {report_title}: {report_link}")
                # Optional: Download the report if it's a file link
                if report_link.endswith(".pdf"):  # Adjust for file types
                    driver.get(report_link)
                    print(f"Downloading {report_title}...")
                    time.sleep(5)  # Wait for download
        else:
            print(f"No reports found on {site_config['name']}.")
    except Exception as e:
        print(f"Error scraping {site_config['name']}: {e}")

# Loop through websites and scrape each
for site in websites:
    scrape_site(site)

# Close the browser
driver.quit()

# To download the file as csv                 -----------------
import csv

results = []

def scrape_site(site_config):
    """Scrape a specific site for reports."""
    global results
    print(f"Scraping {site_config['name']}...")
    driver.get(site_config["url"])
    time.sleep(5)

    try:
        elements = driver.find_elements(By.PARTIAL_LINK_TEXT, site_config["report_keyword"])
        if elements:
            for element in elements:
                report_title = element.text
                report_link = element.get_attribute("href")
                results.append({
                    "site": site_config["name"],
                    "title": report_title,
                    "link": report_link,
                })
    except Exception as e:
        print(f"Error scraping {site_config['name']}: {e}")

# Save results to CSV
with open("quarterly_reports.csv", "w", newline="", encoding="utf-8") as file:
    writer = csv.DictWriter(file, fieldnames=["site", "title", "link"])
    writer.writeheader()
    writer.writerows(results)

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

# Same as above, but to download pdf files

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
import time
import os

# Path to ChromeDriver
driver_path = "path_to_chromedriver"  # Replace with the actual path to your ChromeDriver

# Set up browser options
download_dir = "path_to_download_directory"  # Replace with your preferred download directory
os.makedirs(download_dir, exist_ok=True)  # Create the directory if it doesn't exist

chrome_options = Options()
chrome_options.add_experimental_option("prefs", {
    "download.default_directory": download_dir,  # Set default download directory
    "download.prompt_for_download": False,      # Disable download prompts
    "plugins.always_open_pdf_externally": True, # Open PDFs directly without the browser viewer
})
service = Service(driver_path)
driver = webdriver.Chrome(service=service, options=chrome_options)

# List of websites and configurations
websites = [
    {
        "name": "Site A",
        "url": "https://www.example-a.com/financial-reports",
        "report_keyword": "Quarterly Report",  # Keyword to search for
    },
    {
        "name": "Site B",
        "url": "https://www.example-b.com/reports",
        "report_keyword": "Financial Statement",
    },
    # Add more websites here
]

def scrape_and_download_pdfs(site_config):
    """Scrape a site for PDFs and download them."""
    print(f"Scraping {site_config['name']}...")
    driver.get(site_config["url"])
    time.sleep(5)  # Wait for the page to load

    try:
        # Find all links matching the keyword
        elements = driver.find_elements(By.PARTIAL_LINK_TEXT, site_config["report_keyword"])
        if elements:
            print(f"Found reports on {site_config['name']}:")
            for element in elements:
                report_title = element.text
                report_link = element.get_attribute("href")

                # Check if the link points to a PDF
                if report_link and report_link.endswith(".pdf"):
                    print(f"Downloading {report_title} from {report_link}...")
                    driver.get(report_link)  # Navigate to the PDF link to trigger download
                    time.sleep(5)  # Wait for the download to complete
        else:
            print(f"No reports found on {site_config['name']}.")
    except Exception as e:
        print(f"Error scraping {site_config['name']}: {e}")

# Loop through the websites and scrape each
for site in websites:
    scrape_and_download_pdfs(site)

# Close the browser
driver.quit()

print(f"All downloads are saved in: {download_dir}")

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
